{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 1 - Working with Prompts and LLMs\n",
    "\n",
    "In this workshop, you will learning how to write prompts and feed them into LLMs. You will also be learning how to use different prompt techniques to improve the response from the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and exploring the dataset\n",
    "\n",
    "The workshop will be using [<code>facebook/ExploreToM</code>](https://huggingface.co/datasets/facebook/ExploreToM) dataset from [HuggingFace](https://huggingface.co/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset name\n",
    "dataset_name = \"facebook/ExploreToM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load the following libraries: datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explore the dataset\n",
    "\n",
    "# TODO: Number of rows \n",
    "\n",
    "\n",
    "# TODO: Keys in the dataset\n",
    "\n",
    "\n",
    "# TODO: Feature names\n",
    "\n",
    "\n",
    "# TODO: Display a single row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding and decoding text\n",
    "\n",
    "LLMs with with vectors/tensors. Before passing text into the LLMs to be evaluated, the text have to be first tokenized (encoded) into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load tokenizer\n",
    "\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 Models\n",
    "\n",
    "The <code>flan-t5</code> is a Text-To-Text Transfer Transformer (T5) that is capable of performing zero-shot NLP task such as summary, simple reasoninig, answering questions, etc. \n",
    "\n",
    "Some T5 models from Huggingface\n",
    "- [<code>google/flan-t5-base</code>](https://huggingface.co/google/flan-t5-base)\n",
    "- [<code>google/flan-t5-small</code>](https://huggingface.co/google/flan-t5-small)\n",
    "- [<code>google/flan-t5-xl</code>](https://huggingface.co/google/flan-t5-xl)\n",
    "- [<code>google/flan-t5-xxl</code>](https://huggingface.co/google/flan-t5-xxl) - full model\n",
    "\n",
    "Complete list of [T5 models](https://huggingface.co/models?search=google/flan) on Huggingface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model names\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model_name = \"google/flan-t5-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dba0e7ca41844495a79a3484b7672971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d87a910cb749f08f72dc8d90af557a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec7f3ec372548cd9e6b9291c356e08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe2b90c28d414de9a3510b450aacab25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Create a tokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  600,  1001,  8143,     3, 27779,  1001,  1717,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Tokenize and explore the tokenized text, contents and size\n",
    "# Try different message text\n",
    "\n",
    "message = \"big black bug bleed black blood\"\n",
    "\n",
    "enc_messgage = tokenizer(message, return_tensors='pt')\n",
    "\n",
    "print(enc_messgage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "# Print the encoded message\n",
    "print(len(enc_messgage['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big black bug bleed black blood\n"
     ]
    }
   ],
   "source": [
    "# TODO: Decode the tokenized text\n",
    "dec_message = tokenizer.decode(enc_messgage['input_ids'][0], skip_special_tokens=True)\n",
    "\n",
    "print(dec_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  600,  1001,  8143,     3, 27779,     7,  1001,  1717,     1,     0],\n",
      "        [  255,  1789,  2805,  7300,     7,    30,     8,  2805, 10433,     1],\n",
      "        [ 1704,  4216,     3, 20400,  4418,   147,     8, 19743,  1782,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "   \"big black bug bleeds black blood\",\n",
    "   \"she sell sea shells on the sea shore\",\n",
    "   \"quick brown fox jump over the lazy dog\"\n",
    "]\n",
    "\n",
    "enc_messages = tokenizer(messages, return_tensors='pt', padding=True)\n",
    "\n",
    "print(enc_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "encoding:  tensor([  600,  1001,  8143,     3, 27779,     7,  1001,  1717,     1,     0])\n",
      "len:  10\n",
      "attention:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 0])\n",
      "encoding:  tensor([  255,  1789,  2805,  7300,     7,    30,     8,  2805, 10433,     1])\n",
      "len:  10\n",
      "attention:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "encoding:  tensor([ 1704,  4216,     3, 20400,  4418,   147,     8, 19743,  1782,     1])\n",
      "len:  10\n",
      "attention:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(len(enc_messages['input_ids']))\n",
    "print(len(enc_messages['attention_mask']))\n",
    "\n",
    "for k in range(len(enc_messages['input_ids'])):\n",
    "   print('encoding: ', enc_messages['input_ids'][k])\n",
    "   print('len: ', len(enc_messages['input_ids'][k]))\n",
    "   print('attention: ', enc_messages['attention_mask'][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big black bug bleeds black blood</s><pad>\n"
     ]
    }
   ],
   "source": [
    "dec_message = tokenizer.decode(enc_messages['input_ids'][0])\n",
    "print(dec_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load LLM \n",
    "\n",
    "Create and instance of the Large Language Model (LLM). We  will then create a simple prompt, tokenize the prompt and feed the tokenized prompt to the LLM. The response from the LLM will be decoded to human friendly text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load LLM packages - AutoModelForSeq2SeqLM and GenerationConfig\n",
    "from transformers import AutoModelForSeq2SeqLM, GenerationConfig\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0, 3412,   63,    1]])\n",
      "rainy\n"
     ]
    }
   ],
   "source": [
    "# TODO: Answer simple question\n",
    "\n",
    "enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "#print(enc_prompt)\n",
    "\n",
    "enc_answer = model.generate(enc_prompt['input_ids'])\n",
    "print(enc_answer)\n",
    "\n",
    "answer = tokenizer.decode(enc_answer[0], skip_special_tokens=True)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">  Leslie entered the main tent. Leslie left the main tent. Isabella entered the storage trailer. Isabella moved the stuffed rabbit to the wooden chest, which is also located in the storage trailer. Leslie entered the main tent. Isabella moved the stuffed rabbit to the main tent, leaving the wooden chest in its original location. Isabella told out loud about the festival marketing strategies. Isabella told privately to Colton that Leslie is in the main tent. While this action was happening, Leslie witnessed this action in secret (and only this action).\n",
      ">  The warm glow of string lights illuminated the vibrant colors of the festival grounds, casting a lively atmosphere over rows of booths and attractions. A faint scent of popcorn and sugar wafted through the air, mingling with the distant sounds of laughter and music, as the night's festivities had just begun to unfold. Leslie slipped unnoticed into the main tent, the sounds of the festival outside momentarily muffled by the canvas walls. Leslie slid past the closing canvas flaps, merging with the lively crowd outside, their bright ringmaster attire momentarily camouflaged amidst swirling festivalgoers. The storage trailer stood quietly behind the booths, its door slowly creaking open to admit Isabella. As she entered, the sounds of laughter and music grew duller, replaced by the musty scent of stored supplies and the faint rustle of fabric. As Isabella rearranged the storage trailer's contents, the stuffed rabbit found a new resting place at the bottom of the old wooden chest, its glassy eyes glinting faintly in the dim light. Leslie was swallowed by the main tent's dimly lit interior, briefly allowing the boisterous atmosphere to conceal their movements, before reemerging into the night's festivities outside, ringmaster attire now blending seamlessly into the swirling crowds. As Isabella transferred the stuffed rabbit to the main tent, the lively chatter and song of the festival's attendees momentarily drowned out the sound of her footsteps, leaving the storage trailer - and its wooden chest - quietly behind. The warmth of her voice mingled with the sound of laughter and music, carrying Isabella's thoughts on the festival's marketing prowess out across the grounds, a wafting trail of insight left in her wake. Isabella mouthed a message to Colton, who lingered at the edge of the crowd, her eyes locking briefly onto his as she mouthed, 'He's back in the main tent,' and Colton's eyes flickered, acknowledging the secret message before he left to reposition himself in the rear of the main tent.\n",
      ">  What does Isabella think about Colton's belief on festival marketing strategies? (knows about it / does not know about it)\n",
      ">  does not know about it\n"
     ]
    }
   ],
   "source": [
    "# TODO: Display a short story\n",
    "idx = 100\n",
    "#print(dataset['train'][idx])\n",
    "#for k in dataset['train'].features:   \n",
    "#   print(k)\n",
    "   \n",
    "print('> ', dataset['train'][idx]['story_structure'])\n",
    "print('> ', dataset['train'][idx]['infilled_story'])\n",
    "print('> ', dataset['train'][idx]['question'])\n",
    "print('> ', dataset['train'][idx]['expected_answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: Kaylee entered the hotel lobby. Kaylee moved the silver letter opener to the wooden desk drawer, which is also located in the hotel lobby. While this action was happening, Liam witnessed this action in secret (and only this action). Kaylee left the hotel lobby. Liam entered the hotel lobby. Kaylee entered the hotel lobby. Liam moved the silver letter opener to the leather briefcase, which is also located in the hotel lobby.\n",
      "Question: In which container will Liam search for the silver letter opener?\n",
      "Expected: leather briefcase\n",
      "Response: Kaylee moved the silver letter opener to the wooden desk drawer. Liam witnessed this action in secret (and only this action).\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "story = dataset['train'][idx]['story_structure']\n",
    "question = dataset['train'][idx]['question']\n",
    "expected_answer = dataset['train'][idx]['expected_answer']\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Read the following story:\n",
    "\n",
    "{story}\n",
    "\n",
    "Answer this question regarding the story:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Summarize the following story:\n",
    "\n",
    "{story}\n",
    "\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\" \n",
    "Read the following story and answer the question:\n",
    "\n",
    "{story}\n",
    "\n",
    "Question: Where is Kaylee?\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"Write a short summary for this text: {story}\\n\\nSummary:\"\n",
    "\n",
    "\n",
    "enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "enc_compl = model.generate(enc_prompt['input_ids'], max_new_tokens=512)\n",
    "\n",
    "compl = tokenizer.decode(enc_compl[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'Story: {story}')\n",
    "print(f'Question: {question}')\n",
    "print(f'Expected: {expected_answer}')\n",
    "print(f'Response: {compl}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan Templates\n",
    "\n",
    "Flan has published [prompt templates](https://github.com/google-research/FLAN/blob/main/flan/v2/templates.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Los Angeles fires are threatening to be among the costliest in US history, with the death toll rising to 24\n"
     ]
    }
   ],
   "source": [
    "# TODO: Find a prompt template from Flan promp templates and use that as your prompt.\n",
    "title = \"LA fires death toll rises to 24 as high winds expected\"\n",
    "# Change the title, otherwise the model will return the title!\n",
    "title = \"LA fires death toll rises\"\n",
    "\n",
    "text = \"\"\" \n",
    "Weather forecasters in California are warning fierce winds which fuelled the infernos around Los Angeles are expected to pick up again this week, as fire crews on the ground race to make progress controlling three wildfires.\n",
    "\n",
    "Officials warned that after a weekend of relatively calm winds, the notoriously dry Santa Ana winds would pick up again from Sunday night until Wednesday, reaching speeds of up to 60mph (96km/h).\n",
    "\n",
    "Ahead of the wind's uptick, some progress has been made in stopping the spread of the deadly Palisades and Eaton fires, which are burning on opposite ends of the city. Local firefighters are being assisted by crews from eight other states, as well as Canada and Mexico, who continue to arrive.\n",
    "\n",
    "The LA County medical examiner updated the death toll on Sunday to 24, while officials said earlier at least another 16 remain missing.\n",
    "\n",
    "Sixteen of the dead were found in the Eaton fire zone, while eight were found in the Palisades area.\n",
    "\n",
    "Three conflagrations continue to burn around Los Angeles.\n",
    "\n",
    "The largest fire is the Palisades, which has now burnt through more than 23,000 acres and is 13% contained.\n",
    "\n",
    "The Eaton fire is the second biggest and has burnt through more than 14,000 acres. It is 27% contained.\n",
    "\n",
    "The Hurst fire has grown to 799 acres and has been almost fully contained.\n",
    "\n",
    "The wildfires are on track to be among the costliest in US history.\n",
    "\n",
    "On Sunday, private forecaster Accuweather increased its preliminary estimate of financial losses from the blazes to between $250bn-$275bn.\n",
    "\"\"\"\n",
    "\n",
    "prompt=f\"{title}\\n\\n{text}\\n\\nWrite a one or two sentence summary.\"\n",
    "\n",
    "enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "enc_resp = model.generate(enc_prompt['input_ids'], max_new_tokens=4096)\n",
    "resp = tokenizer.decode(enc_resp[0], skip_special_tokens=True)\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the LLM\n",
    "\n",
    "Use [<code>GenerationConfig</code>](https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationConfig) to change the parameters of the LLM generation. Try different parameters to observe how these changes influences the LLM output.\n",
    "\n",
    "Common parameters\n",
    "- <code>max_new_tokens</code> - The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt. Controls the output length.\n",
    "- <code>temperature</code> - The value used to modulate the next token probabilities. Manipulate the model's output logit.\n",
    "- <code>do_sample</code> - Whether or not to use sampling ; use greedy decoding otherwise. Controls the generation strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use GenerationConfig to change the LLM parameters. Try different values.\n",
    "\n",
    "from transformers import GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story: Kaylee entered the hotel lobby. Kaylee moved the silver letter opener to the wooden desk drawer, which is also located in the hotel lobby. While this action was happening, Liam witnessed this action in secret (and only this action). Kaylee left the hotel lobby. Liam entered the hotel lobby. Kaylee entered the hotel lobby. Liam moved the silver letter opener to the leather briefcase, which is also located in the hotel lobby.\n",
      "Response: Kaylee moved the silver letter opener to the wooden desk drawer. Liam witnessed this action in\n"
     ]
    }
   ],
   "source": [
    "config = GenerationConfig(max_new_token=128, temperature=1, do_sample=False)\n",
    "\n",
    "\n",
    "prompt=f\"Write a short summary for this text: {story}\\n\\nSummary:\"\n",
    "\n",
    "\n",
    "enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "enc_compl = model.generate(enc_prompt['input_ids'], generation_config=config)\n",
    "\n",
    "compl = tokenizer.decode(enc_compl[0], skip_special_tokens=True)\n",
    "\n",
    "print(f'Sto:ry {story}')\n",
    "print(f'Response: {compl}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
