{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Large Language Model - Model\n",
    "\n",
    "In this workshop, you will learn how to fine tune the prompts and the LLMs to enhance and improves its response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch, time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, Trainer, GenerationConfig, TrainingArguments\n",
    "\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'dialogue', 'summary', 'topic'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n",
      "{'train': (12460, 4), 'validation': (500, 4), 'test': (1500, 4)}\n"
     ]
    }
   ],
   "source": [
    "# Load and explore the following datasets\n",
    "# Q: Number of sets? \n",
    "# Q: How many records in each of these sets?\n",
    "# Q: What are the column names?\n",
    "\n",
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "model_name = \"google/flan-t5-small\"\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\n",
      "train_300\n",
      "\n",
      "DIALOGUE\n",
      "#Person1#: Would you like me to show you our new cleaning unit? It's a clever design.\n",
      "#Person2#: Yes, I'd like to see that. What does it clean exactly?\n",
      "#Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system.\n",
      "#Person2#: What does the unit consist of?\n",
      "#Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control.\n",
      "\n",
      "SUMMARY\n",
      "#Person1# introduces a new cleaning unit to #Person2# and explains it.\n",
      "\n",
      "TOPIC\n",
      "cleaning unit introduction\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print a record\n",
    "idx = 300\n",
    "\n",
    "for k, v in dataset['train'][idx].items():\n",
    "   print(f'{k.upper()}\\n{v}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the LLM model\n",
    "\n",
    "In this workshop we will be turning the <code>google/flan-t5-base</code> model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to dump a model's tunable parameters\n",
    "\n",
    "def print_trainable_model_params(model):\n",
    "   trainable_model_params = 0\n",
    "   all_model_params = 0\n",
    "   for _, param in model.named_parameters():\n",
    "      all_model_params += param.numel()\n",
    "      if param.requires_grad:\n",
    "         trainable_model_params += param.numel()\n",
    "   return f\"Trainable parameters: {trainable_model_params}\\nTotal parameters: {all_model_params}\\nPercentable of trainable parameters: {100 * trainable_model_params / all_model_params:.2f}%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 247577856\n",
      "Total parameters: 247577856\n",
      "Percentable of trainable parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of trainable parameters\n",
    "print(print_trainable_model_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the dialogue dataset\n",
    "\n",
    "We will train the model to summarize dialogue by creating a dialogue-summary pair for the LLM to process. The dialogue is the training data and the summary is the label. This is supervized learning.\n",
    "\n",
    "The prompt will be as follows\n",
    "\n",
    "```\n",
    "Summarize the following dialogue.\\n\n",
    "\\n\n",
    "Fred: ...\\n\n",
    "Barney: ...\\n\n",
    "\\n\n",
    "Summary:\\n\n",
    "Summary of the conversation between Fred and Barney\n",
    "```\n",
    "\n",
    "The prompt and the summary will be tokenized for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utitlity function to prepare the data for training \n",
    "# Tokenize function\n",
    "# Need to create a tokenizer before calling this\n",
    "def tokenize_fn(data):\n",
    "   start_prompt = 'Summarize the following dialogue.\\n\\n'\n",
    "   end_prompt = '\\n\\nSummary:'\n",
    "   prompt = [ start_prompt + d + end_prompt for d in data['dialogue'] ]\n",
    "   summary = data['summary']\n",
    "   data['input_ids'] = tokenizer(prompt, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "   data['labels'] = tokenizer(summary, padding=\"max_length\", truncation=True, return_tensors=\"pt\").input_ids\n",
    "   return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aae3d7b1127b484f81bc967439260c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12460 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77140edd37d41da908e9842e9145b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef2465c3e8e4de6bedd2472096f72a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: prepare the data for training with the tokenize_fn function\n",
    "# Tokenize the 3 splits of the dataset: train, validation, test\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fn, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id train_300\n",
      "dialogue #Person1#: Would you like me to show you our new cleaning unit? It's a clever design.\n",
      "#Person2#: Yes, I'd like to see that. What does it clean exactly?\n",
      "#Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system.\n",
      "#Person2#: What does the unit consist of?\n",
      "#Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control.\n",
      "summary #Person1# introduces a new cleaning unit to #Person2# and explains it.\n",
      "topic cleaning unit introduction\n",
      "input_ids [12198, 1635, 1737, 8, 826, 7478, 5, 1713, 345, 13515, 536, 4663, 10, 5328, 25, 114, 140, 12, 504, 25, 69, 126, 2327, 1745, 58, 94, 31, 7, 3, 9, 13183, 408, 5, 1713, 345, 13515, 357, 4663, 10, 2163, 6, 27, 31, 26, 114, 12, 217, 24, 5, 363, 405, 34, 1349, 1776, 58, 1713, 345, 13515, 536, 4663, 10, 94, 47, 88, 7, 8, 23915, 326, 66, 8, 1946, 1467, 3, 18, 8, 8720, 7, 6, 3, 28501, 672, 5, 3, 18, 11, 258, 1299, 7, 34, 223, 139, 8, 358, 5, 1713, 345, 13515, 357, 4663, 10, 363, 405, 8, 1745, 5608, 13, 58, 1713, 345, 13515, 536, 4663, 10, 1548, 6, 34, 31, 7, 6171, 192, 16007, 6, 80, 21, 8, 13086, 23915, 11, 80, 21, 8, 1349, 23915, 6, 3, 9, 5013, 11, 3, 9, 9834, 1745, 5, 3359, 6, 11, 132, 31, 7, 3, 9, 9243, 358, 11, 3, 9, 4191, 5, 94, 31, 7, 66, 6478, 57, 3, 9, 276, 6480, 358, 6, 24, 5024, 21, 10272, 3, 20641, 4330, 5, 20698, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels [1713, 345, 13515, 536, 4663, 4277, 7, 3, 9, 126, 2327, 1745, 12, 1713, 345, 13515, 357, 4663, 11, 3, 9453, 34, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "\n",
      "\n",
      "Summarize the following dialogue. #Person1#: Would you like me to show you our new cleaning unit? It's a clever design. #Person2#: Yes, I'd like to see that. What does it clean exactly? #Person1#: It washes the solvent off all the metal parts - the blades, trays etc. - and then sends it back into the system. #Person2#: What does the unit consist of? #Person1#: Well, it's basically two tanks, one for the dirty solvent and one for the clean solvent, a pump and a washing unit. Oh, and there's a cooling system and a filter. It's all controlled by a PLC system, that stands for Process Logic Control. Summary:\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify prepared data\n",
    "for _, k in enumerate(tokenized_dataset['train'][idx]):\n",
    "   print(k, tokenized_dataset['train'][idx][k])\n",
    "\n",
    "dec_prompt = tokenizer.decode(tokenized_dataset['train'][idx]['input_ids'], skip_special_tokens=True)\n",
    "\n",
    "print('\\n\\n')\n",
    "print(dec_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove id, dialogue, summary and topic columns from dataset. We only want input_ids and labels\n",
    "drop_cols = [ 'id', 'dialogue', 'summary', 'topic' ]\n",
    "clean_tokenized_dataset = tokenized_dataset.remove_columns(drop_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids =  [12198, 1635, 1737, 8, 826, 7478, 5, 1713, 345, 13515, 536, 4663, 10, 5328, 25, 114, 140, 12, 504, 25, 69, 126, 2327, 1745, 58, 94, 31, 7, 3, 9, 13183, 408, 5, 1713, 345, 13515, 357, 4663, 10, 2163, 6, 27, 31, 26, 114, 12, 217, 24, 5, 363, 405, 34, 1349, 1776, 58, 1713, 345, 13515, 536, 4663, 10, 94, 47, 88, 7, 8, 23915, 326, 66, 8, 1946, 1467, 3, 18, 8, 8720, 7, 6, 3, 28501, 672, 5, 3, 18, 11, 258, 1299, 7, 34, 223, 139, 8, 358, 5, 1713, 345, 13515, 357, 4663, 10, 363, 405, 8, 1745, 5608, 13, 58, 1713, 345, 13515, 536, 4663, 10, 1548, 6, 34, 31, 7, 6171, 192, 16007, 6, 80, 21, 8, 13086, 23915, 11, 80, 21, 8, 1349, 23915, 6, 3, 9, 5013, 11, 3, 9, 9834, 1745, 5, 3359, 6, 11, 132, 31, 7, 3, 9, 9243, 358, 11, 3, 9, 4191, 5, 94, 31, 7, 66, 6478, 57, 3, 9, 276, 6480, 358, 6, 24, 5024, 21, 10272, 3, 20641, 4330, 5, 20698, 10, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels =  [1713, 345, 13515, 536, 4663, 4277, 7, 3, 9, 126, 2327, 1745, 12, 1713, 345, 13515, 357, 4663, 11, 3, 9453, 34, 5, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Verify dataset again\n",
    "\n",
    "print('input_ids = ', clean_tokenized_dataset['train'][idx]['input_ids'])\n",
    "print('labels = ', clean_tokenized_dataset['train'][idx]['labels'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune model with pre-processed dataset\n",
    "\n",
    "We will use [<code>Trainer</code>](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#api-reference%20][%20transformers.Trainer) to train the original model. The training result will be written out. The trainer will be configure with [<code>TrainingArgument</code>](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available:  False\n"
     ]
    }
   ],
   "source": [
    "# CUDA information\n",
    "# We have install the torch CPU version\n",
    "# pip3 install torch==2.5.1+cpu --index-url https://download.pytorch.org/whl/cpu\n",
    "# To install CUDA version\n",
    "# pip3 install torch\n",
    "\n",
    "print('CUDA available: ', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "   print('B16 supported: ', torch.cuda.is_bf16_supported())\n",
    "   torch.cuda.set_device(0)\n",
    "   print('Current device: ', torch.cuda.current_device())\n",
    "   print('CUDA device name: ', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the LLM Model with Low-Rank Adaptation (LoRA) / Parameter Efficient Fine Tuning (PEFT)\n",
    "\n",
    "We will add a LoRA adapter to the LLM (flan-t5-base) and train the adapter. The original LLM will be frozen. The adapter can be combined with the original LLM during inferencing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "   r=32,\n",
    "   lora_alpha=32,\n",
    "   target_modules=[ 'q', 'v' ],\n",
    "   lora_dropout=0.05,\n",
    "   bias='none',\n",
    "   task_type=TaskType.SEQ_2_SEQ_LM\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add LoRA to the LLM model to be trained\n",
    "# load the original model. \n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name, torch_dtype = torch.bfloat16)\n",
    "\n",
    "# create the LoRA model \n",
    "lora_model = get_peft_model(original_model, lora_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA:  Trainable parameters: 3538944\n",
      "Total parameters: 251116800\n",
      "Percentable of trainable parameters: 1.41%\n",
      "Original model:  Trainable parameters: 247577856\n",
      "Total parameters: 247577856\n",
      "Percentable of trainable parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Print number of parameters, compare LoRA to the original model\n",
    "print('LoRA: ', print_trainable_model_params(lora_model))\n",
    "\n",
    "# LoRA fuses the adaptation to the original model\n",
    "#print('Original model: ', print_trainable_model_params(original_model))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "print('Original model: ', print_trainable_model_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train model with LoRA\n",
    "output_dir = f'peft-dialog-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "lora_training_args = TrainingArguments(\n",
    "   output_dir=output_dir, \n",
    "   auto_find_batch_size=True,\n",
    "   learning_rate=1e-3,\n",
    "   num_train_epochs=1,\n",
    "   logging_first_step=1,\n",
    "   max_steps=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create trainer and train model\n",
    "lora_trainer = Trainer(\n",
    "   model = lora_model,\n",
    "   args = lora_training_args,\n",
    "   train_dataset = clean_tokenized_dataset['train'],\n",
    "   eval_dataset = clean_tokenized_dataset['validation']\n",
    ")\n",
    "\n",
    "# Start the training\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save trained model\n",
    "lora_model_path = 'lora_dialogue_summary_checkpoint'\n",
    "\n",
    "# Save the model\n",
    "lora_trainer.model.save_pretrain(lora_model_path)\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(lora_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use a trained LoRA model\n",
    "\n",
    "The training will take a few hours and over many iterations.\n",
    "\n",
    "For the purpose of this workshop we use a save model [intotheverse/peft-dialogue-summary-checkpoint](https://huggingface.co/intotheverse/peft-dialogue-summary-checkpoint)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 0\n",
      "Total parameters: 251116800\n",
      "Percentable of trainable parameters: 0.00%\n"
     ]
    }
   ],
   "source": [
    "#TODO: Load the original model and add the pre-trained LoRA adaptation to the model\n",
    "peft_dialogue_summary_checkpoint = 'intotheverse/peft-dialogue-summary-checkpoint'\n",
    "\n",
    "# Load the base model\n",
    "original_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "   model_name, torch_dtype = torch.bfloat16\n",
    ")\n",
    "\n",
    "# Load the LoRA model\n",
    "lora_model = PeftModel.from_pretrained(\n",
    "               original_model, # the original model\n",
    "               peft_dialogue_summary_checkpoint, # the LoRA adaptation\n",
    "               torch_dtype=torch.bfloat16, # quantization\n",
    "               is_trainable=False) # not trainable \n",
    "\n",
    "print(print_trainable_model_params(lora_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LoRA model\n",
      "Label: $#Person2# tells David the plan for a tour and #Person2# will celebrate #Person2#'s brother's fortieth birthday when at Salt Lake City.\n",
      "Original: $David and his brother are going on a four day drive to Salt Lake City this Friday.\n",
      "LoRA: $David is planning a tour for his vacation. He will start out from Long Island this Friday and\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LoRA model with a single sample\n",
    "# Pick a sample from the test dataset, \n",
    "# compare the completions between model, lora_model\n",
    "\n",
    "idx = 500\n",
    "\n",
    "def create_prompt(data):\n",
    "   start_prompt = 'Summarize the following dialogue.\\n\\n'\n",
    "   end_prompt = '\\n\\nSummary:'\n",
    "   return f\"{start_prompt}{data['dialogue']}{end_prompt}\"\n",
    "\n",
    "#print(create_prompt(dataset['test'][idx]))\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Create and encode the prompt\n",
    "prompt = create_prompt(dataset['test'][idx])\n",
    "enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "# Get completion from the original model\n",
    "comp_original_model = model.generate(input_ids=enc_prompt['input_ids'])\n",
    "comp_original_model_resp = tokenizer.decode(comp_original_model[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Calling LoRA model\")\n",
    "comp_lora_model = lora_model.generate(input_ids=enc_prompt['input_ids'])\n",
    "comp_lora_model_resp = tokenizer.decode(comp_lora_model[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Label: ${dataset['test'][idx]['summary']}\")\n",
    "\n",
    "print(f\"Original: ${comp_original_model_resp}\")\n",
    "\n",
    "print(f\"LoRA: ${comp_lora_model_resp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx:  300\n",
      "idx:  301\n",
      "idx:  302\n",
      "idx:  303\n",
      "idx:  304\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compare LoRA against the original model \n",
    "\n",
    "dialogues = []\n",
    "summaries = []\n",
    "orig_model_summaries = []\n",
    "lora_model_summaries = []\n",
    "config = GenerationConfig(max_new_tokens=512)\n",
    "\n",
    "for idx in range(300, 305):\n",
    "   print('idx: ', idx)\n",
    "   # create and encode prompt\n",
    "   prompt = create_prompt(dataset['test'][idx])\n",
    "   enc_prompt = tokenizer(prompt, return_tensors='pt')\n",
    "\n",
    "   # Get completion from the original model\n",
    "   comp_original_model = model.generate(input_ids=enc_prompt['input_ids'], generation_config=config)\n",
    "   comp_original_model_resp = tokenizer.decode(comp_original_model[0], skip_special_tokens=True)\n",
    "\n",
    "   comp_lora_model = lora_model.generate(input_ids=enc_prompt['input_ids'], generation_config=config)\n",
    "   comp_lora_model_resp = tokenizer.decode(comp_lora_model[0], skip_special_tokens=True)\n",
    "\n",
    "   summaries.append(dataset['test'][idx]['summary'])\n",
    "   orig_model_summaries.append(comp_original_model_resp)\n",
    "   lora_model_summaries.append(comp_lora_model_resp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>original_summaries</th>\n",
       "      <th>lora_summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#Person1# is crazy for Trump and voted for him. #Person2# doesn't agree with #Person1# on Trump and will vote for Biden.</td>\n",
       "      <td>Person1 is proud of Trump, and is happy if he can be re-elected.</td>\n",
       "      <td>#Person1# and #Person2# are happy if Trump could be our President again. #Person2# believes Trump will make America great again but #Person1# doesn't think he is the right person. #Person2# will vote for Biden instead.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#Person1# is a crazy fan of Trump and wants him to be re-elected. #Person2# will vote for Biden.</td>\n",
       "      <td>Person1 is proud of Trump, and is happy if he can be re-elected.</td>\n",
       "      <td>#Person1# and #Person2# are happy if Trump could be our President again. #Person2# believes Trump will make America great again but #Person1# doesn't think he is the right person. #Person2# will vote for Biden instead.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#Person1# is crazy for Trump and voted for him but #Person2# will vote for Biden.</td>\n",
       "      <td>Person1 is proud of Trump, and is happy if he can be re-elected.</td>\n",
       "      <td>#Person1# and #Person2# are happy if Trump could be our President again. #Person2# believes Trump will make America great again but #Person1# doesn't think he is the right person. #Person2# will vote for Biden instead.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#Person1# doesn't know how to use the ATM. #Person2# teaches #Person1# step by step.</td>\n",
       "      <td>#Person1#: I need to use the ATM. #Person2#: OK.</td>\n",
       "      <td>#Person1# needs to use the ATM. #Person2# helps #Person1# figure out how to use it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Person1# doesn't know how to use an ATM. #Person2# teaches #Person1#.</td>\n",
       "      <td>#Person1#: I need to use the ATM. #Person2#: OK.</td>\n",
       "      <td>#Person1# needs to use the ATM. #Person2# helps #Person1# figure out how to use it.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                      label  \\\n",
       "0  #Person1# is crazy for Trump and voted for him. #Person2# doesn't agree with #Person1# on Trump and will vote for Biden.   \n",
       "1                          #Person1# is a crazy fan of Trump and wants him to be re-elected. #Person2# will vote for Biden.   \n",
       "2                                         #Person1# is crazy for Trump and voted for him but #Person2# will vote for Biden.   \n",
       "3                                      #Person1# doesn't know how to use the ATM. #Person2# teaches #Person1# step by step.   \n",
       "4                                                    #Person1# doesn't know how to use an ATM. #Person2# teaches #Person1#.   \n",
       "\n",
       "                                                 original_summaries  \\\n",
       "0  Person1 is proud of Trump, and is happy if he can be re-elected.   \n",
       "1  Person1 is proud of Trump, and is happy if he can be re-elected.   \n",
       "2  Person1 is proud of Trump, and is happy if he can be re-elected.   \n",
       "3                  #Person1#: I need to use the ATM. #Person2#: OK.   \n",
       "4                  #Person1#: I need to use the ATM. #Person2#: OK.   \n",
       "\n",
       "                                                                                                                                                                                                               lora_summaries  \n",
       "0  #Person1# and #Person2# are happy if Trump could be our President again. #Person2# believes Trump will make America great again but #Person1# doesn't think he is the right person. #Person2# will vote for Biden instead.  \n",
       "1  #Person1# and #Person2# are happy if Trump could be our President again. #Person2# believes Trump will make America great again but #Person1# doesn't think he is the right person. #Person2# will vote for Biden instead.  \n",
       "2  #Person1# and #Person2# are happy if Trump could be our President again. #Person2# believes Trump will make America great again but #Person1# doesn't think he is the right person. #Person2# will vote for Biden instead.  \n",
       "3                                                                                                                                         #Person1# needs to use the ATM. #Person2# helps #Person1# figure out how to use it.  \n",
       "4                                                                                                                                         #Person1# needs to use the ATM. #Person2# helps #Person1# figure out how to use it.  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the df for comparison\n",
    "zip_summaries = list(zip(summaries, orig_model_summaries, lora_model_summaries))\n",
    "\n",
    "pd.options.display.max_colwidth = 500\n",
    "\n",
    "cols = [ 'label', 'original_summaries', 'lora_summaries']\n",
    "df = pd.DataFrame(zip_summaries, columns=cols)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models with ROUGE/Bleu metrics\n",
    "\n",
    "Recall-Oriented Understudy for Gisting Evaluate ([ROUGE](https://pub.aimind.so/unveiling-the-power-of-rouge-metrics-in-nlp-b6d3f96d3363)) is a set of metrics used to evaluate the quality of machine-generated text, such as summaries and translations. ROUGE metrics compare the generated text to a human-written reference and measure the overlap between the two. \n",
    "\n",
    "The metrics range between 0 and 1, with higher scores indicating higher similarity between the baseline and generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model results\n",
      "{'rouge1': 0.3918246504453401, 'rouge2': 0.23250770631176104, 'rougeL': 0.39355625217694185, 'rougeLsum': 0.39052594914663874}\n",
      "\n",
      "LoRA model results\n",
      "{'rouge1': 0.45118418806045585, 'rouge2': 0.20837735849056602, 'rougeL': 0.3652181039604975, 'rougeLsum': 0.36466080685147617}\n"
     ]
    }
   ],
   "source": [
    "# TODO: create ROUGE\n",
    "rouge = evaluate.load('rouge')\n",
    "\n",
    "orig_model_results = rouge.compute(\n",
    "   references=summaries,\n",
    "   predictions=orig_model_summaries,\n",
    "   use_stemmer=True\n",
    ")\n",
    "\n",
    "lora_model_results = rouge.compute(\n",
    "   references=summaries,\n",
    "   predictions=lora_model_summaries,\n",
    "   use_stemmer=True\n",
    ")\n",
    "\n",
    "print('ROUGE - Original model results')\n",
    "print(orig_model_results)\n",
    "\n",
    "print()\n",
    "\n",
    "print('ROUGE - LoRA model results')\n",
    "print(lora_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU - Original model results\n",
      "{'bleu': 0.14406990484132043, 'precisions': [0.5189873417721519, 0.25675675675675674, 0.17391304347826086, 0.109375], 'brevity_penalty': 0.6420828237327826, 'length_ratio': 0.6929824561403509, 'translation_length': 79, 'reference_length': 114}\n",
      "\n",
      "BLEU - LoRA model results\n",
      "{'bleu': 0.21926781650570026, 'precisions': [0.43157894736842106, 0.2756756756756757, 0.18888888888888888, 0.10285714285714286], 'brevity_penalty': 1.0, 'length_ratio': 1.6666666666666667, 'translation_length': 190, 'reference_length': 114}\n"
     ]
    }
   ],
   "source": [
    "# TODO: create Bleu\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "orig_model_results = bleu.compute(\n",
    "   references=summaries,\n",
    "   predictions=orig_model_summaries,\n",
    ")\n",
    "\n",
    "lora_model_results = bleu.compute(\n",
    "   references=summaries,\n",
    "   predictions=lora_model_summaries\n",
    ")\n",
    "\n",
    "print('BLEU - Original model results')\n",
    "print(orig_model_results)\n",
    "\n",
    "print()\n",
    "\n",
    "print('BLEU - LoRA model results')\n",
    "print(lora_model_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
