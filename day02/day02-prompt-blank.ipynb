{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning Large Language Model - Prompts\n",
    "\n",
    "In this workshop, you will learn how to fine tune the prompts and the LLMs to enhance and improves its response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Exploring the dataset\n",
    "\n",
    "In this workshop we will be using [<code>knkarthick/dialogsum</code>](https://huggingface.co/datasets/knkarthick/dialogsum) dataset from [HuggingFace](https://huggingface.co/). The dataset contains manually labelled summary and topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load and explore the following datasets\n",
    "# Q: Number of sets? \n",
    "# Q: How many records in each of these sets?\n",
    "# Q: What are the column names?\n",
    "\n",
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "print(dataset)\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 100 \n",
    "\n",
    "for k, v in dataset['train'][idx].items():\n",
    "   print(f'{k.upper()}\\n{v}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a prompt to summarize the dialogue from the training dataset. Use the google/flan-t5-base LLM.  \n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning the prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot prompt\n",
    "\n",
    "A instruction prompt for the LLM to perform a task without having seen any examples. \n",
    "\n",
    "Characteristics of zero-shot prompts include\n",
    "- <b>No provided examples</b> The prompt does not include any explicit examples of the desired input or output\n",
    "- <b>Direct instruction</b> The prompt describes what is expected of the LLM\n",
    "- <b>Relies on pre-training</b> Relies only on the data that the LLM is trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a Zero-Shot prompt \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a Zero Shot prompt using Flan prompt template\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One and Few Short Prompt\n",
    "\n",
    "Refers to the prompting technique where the model is given 1 example of the task. The example is used to help the LLM to understand the task's requirements and to generate appropriate response.\n",
    "\n",
    "Characteristics of a one-shot prompt include\n",
    "- <b>Single example</b> - The prompt includes a single input/output pair example\n",
    "- <b>Instruction and query</b> - The provided example will be followed by a query that the LLM needs to respond to in the same format as the example\n",
    "- <b>Task guidance</b> - The example serves to guide the LLM to give the desired response\n",
    "\n",
    "FLAN uses the following template format for one/few shot prompts\n",
    "```\n",
    "<input_prefix>\n",
    "{example_0_inputs}\n",
    "\\n\\n\n",
    "<target_prefix>\n",
    "{example_0_targets}\n",
    "\\n\\n\\n\n",
    "<input_prefix>\n",
    "{example_1_inputs}\n",
    "\\n\\n\n",
    "<target_prefix>\n",
    "{example_1_targets}\n",
    "\\n\\n\\n\n",
    "<input_prefix>\n",
    "{actual_inputs}\n",
    "\\n\\n\n",
    "<target_prefix>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create examplars\n",
    "def mk_examplars(dataset, idxs):\n",
    "   prompt = \"\"\n",
    "   for _, i in enumerate(idxs):\n",
    "      dialogue = dataset['train'][i]['dialogue']\n",
    "      summary = dataset['train'][i]['summary']\n",
    "      #summary = dataset['train'][i]['topic']\n",
    "      prompt += f\"Summarize this article:\\n\\n{dialogue}\\n\\nSummary:\\n{summary}\\n\\n\\n\"\n",
    "   return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
